{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34adc7ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5eadd68e68aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#定义神经网络输入的真实数据（理解为机器学习中的特征features）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#定义神经网络输入的真实数据（理解为机器学习中的特征features）\n",
    "x_data = np.linspace(-1, 1, 300)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34992831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建神经网络的隐藏层\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    '''\n",
    "    :param inputs: 输入值\n",
    "    :param in_size: 输入值的大小\n",
    "    :param out_size: 输出值的大小，也可以理解为该层神经元个数\n",
    "    :param activation_function: 激活函数\n",
    "    :return: outputs\n",
    "    '''\n",
    "\n",
    "    # 定义Weights为一个 in_size行 out_size列 的随机变量矩阵；weights就是权重\n",
    "    Weights = tf.Variable(tf.compat.v1.random_normal([in_size, out_size]))\n",
    "    # biases一般不推荐0, 所以在0的基础上加0.1；biases就是偏置\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    # 矩阵相乘；实现公式 wx+b。\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "\n",
    "#xs是输入数据的格式，1是输入数据的大小，10是该层里有10个神经元（或者是该层的输出个数）\n",
    "hide_1 = add_layer(xs, 1, 10, activation_function=tf.nn.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义神经网络数据输出的格式\n",
    "ys = tf.compat.v1.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c308412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建神经网络的输出层\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    '''\n",
    "    :param inputs: 输入值\n",
    "    :param in_size: 输入值的大小\n",
    "    :param out_size: 输出值的大小，也可以理解为该层神经元个数\n",
    "    :param activation_function: 激活函数\n",
    "    :return: outputs\n",
    "    '''\n",
    "    # 定义Weights为一个 in_size行 out_size列 的随机变量矩阵；weights就是权重\n",
    "    Weights = tf.Variable(tf.compat.v1.random_normal([in_size, out_size]))\n",
    "    # biases一般不推荐0, 所以在0的基础上加0.1；biases就是偏置\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    # 矩阵相乘；实现公式 wx+b。\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "#hide_1就是隐藏层的结果，10就是hide_1的大小，1代表最后输出1个结果\n",
    "prediction = add_layer(hide_1, 10, 1, activation_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建图形，画图初始化定义\n",
    "fig = plt.figure()  # 生成图片框\n",
    "ax = fig.add_subplot(1, 1, 1)  # 做一个连续性的画图需要用到add_subplot函数\n",
    "ax.scatter(x_data, y_data)  # plot真实的数据\n",
    "plt.ion()   #本次运行需注释，全局运行不需注释\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fa14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义神经网络数据输入的格式\n",
    "xs = tf.compat.v1.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba60b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算损失函数的最小值\n",
    "train_step = tf.compat.v1.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练网络，并在画图中查看结果\n",
    "for i in range(1000):\n",
    "    sess.run(train_step, feed_dict={xs:x_data, ys:y_data})\n",
    "    if i % 50 == 0:\n",
    "        # print(sess.run(loss, feed_dict={xs:x_data, ys:y_data}))\n",
    "        # 图形显示预测数据\n",
    "        try:\n",
    "            # 如果想连续plot出线，就需要把之前的线去除掉，否则太多的线看不清最后的结果\n",
    "            ax.lines.remove(lines[0])\n",
    "        except:\n",
    "            pass\n",
    "        # 定义预测数据\n",
    "        prediction_value = sess.run(prediction, feed_dict={xs:x_data})\n",
    "        # 用红色、宽度为5的线来显示我们的预测数据和输入之间的关系，并暂停0.1s\n",
    "        lines = ax.plot(x_data, prediction_value, 'r-', lw=5)\n",
    "        plt.pause(0.1)\n",
    "plt.pause(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e3fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化tensorflow定义的参数和会话\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576cd6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数（损失函数就是真实值和预测值之间的均方差）\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), axis=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e8fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
